{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "This notebook contains the implementation for a practical exercise on perceptron studies found on an [Artificial Neural Network book](https://www.amazon.com.br/Neurais-Artificiais-Engenharia-Ci%C3%AAncias-Aplicadas/dp/8588098539). After a brief explanation of how a perceptron model works, a training algorithm is implemented and evaluated on a given dataset.  \n",
    "\n",
    "## The perceptron model\n",
    "\n",
    "A perceptron is the simplest form of a neural network used to classification patterns that are linearly separable. An illustration of the perceptron model is shown in the following picture.\n",
    "\n",
    "<img src=\"Figures/perceptron.png\">\n",
    "\n",
    "Basically, it consists of a single neuron with adjustable sinaptic weights and bias. Let $x \\in R^{m+1}$ be an extended version of an input signal $x_i \\in R^{m}$ by adding a fixed input $x_0 = +1$ (for practical implementations). Conversely, let $w \\in R^{m+1}$ be an extension of a randomly initialized weight vector $w_i \\in R^m$ by adding a bias element $b$. \n",
    "\n",
    "In this case, a signal $u$ is set as the following equation: \n",
    "\n",
    "$\\begin{equation}    \n",
    "    u = \\sum_{k = 1}^m x_k \\cdot w_k - b = w_i^T \\cdot x_i - b = w^T \\cdot x\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "#### Activation Function\n",
    "\n",
    "In this model, the $u$ signal goes through an activation function $\\sigma(\\cdot)$ which has several sorts of flavours: \n",
    "\n",
    "| Activation Type | Equation | \n",
    "| ----------------- | -------------------------| \n",
    "| linear $\\sigma(x)$ | $\\sigma(x) = x$ | \n",
    "| unipolar step (hard limiter) | $\\sigma(x) = \\left\\{ \\begin{array}{ll} 1  & \\mbox{if } x \\geq 0 \\\\ 0 & \\mbox{if } x < 0 \\end{array}\\right.$ |  \n",
    "| bipolar step | $\\sigma(x) = \\left\\{\\begin{array}{ll} 1  & \\mbox{if } x \\geq 0 \\\\ -1 & \\mbox{if } x < 0 \\end{array} \\right.$ | \n",
    "| logistic | $\\sigma(x, \\beta) = \\frac{1}{1+e^{-\\beta x}}$ | \n",
    "| tanh | $\\sigma(x, \\beta) = \\frac{1-e^{-\\beta x}}{1+e^{-\\beta x}}$ | \n",
    "| relu | $\\sigma(x) = \\left\\{\\begin{array}{ll} x  & \\mbox{if } x \\geq 0 \\\\ 0 & \\mbox{if } x < 0 \\end{array} \\right.$ |\n",
    "\n",
    "The logistic and tanh activation functions are said to be in the sigmoid group and they are largely used on regression problems. For classification, the unipolar or bipolar steps are usually applied given their output are finite integers, which can be mapped to classes. \n",
    "\n",
    "It is essential to be aware that, since an activation function has an dinamyc range of operation, both the input and output signals should be preprocessed in a way to also limit its range within that of the activation function's. For example, by using the bipolar step, the input signal should range between $-1$ and $+1$. \n",
    "\n",
    "#### Setting the output\n",
    "\n",
    "Once an activation function is set, the output signal $y$ is defined as \n",
    "\n",
    "$y = \\sigma(u)$\n",
    "\n",
    "The single-neuron perceptron model is characterized by identifying linearly separable classes because, by using a bipolar step activation function, the classes are predicted by the following equation.\n",
    "\n",
    "\n",
    "$y = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            +1  & \\mbox{if } u = w_i^Tx_i - b \\geq 0\\\\\n",
    "            -1 & \\mbox{if } u = w_i^Tx_i - b < 0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$\n",
    "\n",
    "Therefore, by training a single-neuron perceptron model, we are defining a hyperplan defined by $w_i^Tx_i - b = 0$ which separates both classes on $R^m$. \n",
    "\n",
    "#### Training algorithm\n",
    "\n",
    "In order to present a training algorithm, this notebook uses a dataset extracted from the aforementioned [Artificial Neural Network book](https://www.amazon.com.br/Neurais-Artificiais-Engenharia-Ci%C3%AAncias-Aplicadas/dp/8588098539) which is stored in the *./Datasets* folder. \n",
    "\n",
    "The training set contains information of 3 features extracted from a oil destilation process and 1 target value indicating whether registers belong to one of 2 classes {P1 and P2}, denoted by [-1, 1] respectively. The test set contains only the features of another set of data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (30, 4)\n",
      "Test set shape: (10, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.11</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.45</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.09</td>\n",
       "      <td>0.69</td>\n",
       "      <td>12.07</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.26</td>\n",
       "      <td>1.15</td>\n",
       "      <td>7.80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.64</td>\n",
       "      <td>1.02</td>\n",
       "      <td>7.04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1    x2     x3  target\n",
       "0 -0.65  0.11   4.00    -1.0\n",
       "1 -1.45  0.89   4.40    -1.0\n",
       "2  2.09  0.69  12.07    -1.0\n",
       "3  0.26  1.15   7.80     1.0\n",
       "4  0.64  1.02   7.04     1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./Datasets/ex3_6_train.tsv', sep='\\t')\n",
    "df_train.drop(['sample'], axis=1, inplace=True)\n",
    "df_test = pd.read_csv('./Datasets/ex3_6_test.tsv', sep='\\t')\n",
    "df_test.drop(['sample'], axis=1, inplace=True)\n",
    "print (\"Train set shape: {}\\nTest set shape: {}\".format(df_train.shape, df_test.shape))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.78</td>\n",
       "      <td>1.13</td>\n",
       "      <td>5.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.56</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.78</td>\n",
       "      <td>1.06</td>\n",
       "      <td>8.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.80</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1    x2    x3\n",
       "0 -0.37  0.06  5.99\n",
       "1 -0.78  1.13  5.59\n",
       "2  0.30  0.56  5.82\n",
       "3  0.78  1.06  8.07\n",
       "4  0.16  0.80  6.30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = df_train.drop(['target'], axis=1).values, df_train['target'].values\n",
    "x_test = df_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, activation = 'tanh', learning_rate = 0.01, seed = None, beta = None): \n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.x = None\n",
    "        self.x_pred = None\n",
    "        self.w = None        \n",
    "        self.beta = beta\n",
    "        self.g = self.get_activation(self.activation, self.beta)\n",
    "        \n",
    "    def get_activation(self, activation, beta = None):\n",
    "        \"\"\"Returns an activation function\n",
    "            :param activation (str): the name of the function \n",
    "                ['linear', 'unipolar_step', 'bipolar_step', \n",
    "                'logistic', 'simmetric_ramp', 'tanh', 'relu']\n",
    "            :return (lambda function): the implemented activation function\n",
    "        \"\"\"\n",
    "        if activation == 'linear':\n",
    "            g = lambda x: x\n",
    "        elif activation == 'unipolar_step':\n",
    "            g = lambda x: 1 if x >= 0 else 0\n",
    "        elif activation == 'bipolar_step':\n",
    "            g = lambda x: 1 if x >= 0 else -1\n",
    "        elif activation == 'logistic':\n",
    "            g = lambda x, beta: 1/(1 + np.exp(-beta*x))\n",
    "        elif activation == 'simmetric_ramp':\n",
    "            g = lambda x, beta: x if x > -beta or x < beta else beta\n",
    "        elif activation == 'tanh':\n",
    "            g = lambda x, beta: (1 - np.exp(-beta*x))/(1 + np.exp(-beta*x))\n",
    "        elif activation == 'relu':\n",
    "            g = lambda x: x if x > 0 else 0\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        return g\n",
    "        \n",
    "    def train(self, features, target, max_epochs = 30):\n",
    "        \"\"\" Trains a single neuron perceptron model.\n",
    "            :param features (np.array): an array containg training examples and its features\n",
    "            :param target (np.array): the true values of the output \n",
    "            :max_epochs (int): the maximum number of epochs to train the algorithm\n",
    "        \"\"\"\n",
    "        # Appending a bias constant to the features array\n",
    "        self.x = np.array([np.concatenate(([1], i)) for i in features])\n",
    "        \n",
    "        # Initializing the weights with a random uniform function (0, 1)\n",
    "        self.w = np.array([random.uniform(0,1) for i in np.arange(self.x.shape[1]-1)])\n",
    "        self.w = np.concatenate(([-1], self.w))\n",
    "        \n",
    "        epoch = 1\n",
    "        print (\"Epoch {} >> W = {}\".format(epoch, self.w))\n",
    "        \n",
    "        # Starting training until max_epochs is reached or no error is found\n",
    "        keep_training = True\n",
    "        while (keep_training):          \n",
    "            keep_training = False            \n",
    "            for index, sample in enumerate(self.x):\n",
    "                u = np.dot(sample, self.w)\n",
    "                y = self.g(u)                \n",
    "                if (y != target[index]):\n",
    "                    self.w = self.w + self.learning_rate*(target[index]-y)*sample\n",
    "                    keep_training = True\n",
    "            epoch += 1            \n",
    "            if epoch > max_epochs:\n",
    "                keep_training = False\n",
    "        print (\"Epoch {} >> W = {}\".format(epoch, self.w))\n",
    "        \n",
    "    def predict(self, features):\n",
    "        \"\"\" Predicts the output of a set of test features from a pre-trained model\n",
    "            :params features (np.array): the test set of features\n",
    "            :return y_pred (np.array): the predicted output\n",
    "        \"\"\"\n",
    "        self.x_pred = np.array([np.concatenate(([1], i)) for i in features])\n",
    "        u = np.dot(self.x_pred,self.w)\n",
    "        self.y_pred = list()\n",
    "        for u_i in u:\n",
    "            self.y_pred.append(self.g(u_i))\n",
    "        self.y_pred = np.array(self.y_pred)\n",
    "        return self.y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Starting training  1\n",
      "Epoch 1 >> W = [-1.          0.30750558  0.24407762  0.11136984]\n",
      "Epoch 466 >> W = [ 3.14        1.60750558  2.53687762 -0.73363016]\n",
      "Predictions:  [-1  1  1  1  1  1 -1  1 -1 -1]\n",
      "\n",
      "### Starting training  2\n",
      "Epoch 1 >> W = [-1.          0.85632945  0.97428431  0.38397214]\n",
      "Epoch 457 >> W = [ 3.06        1.56512945  2.51268431 -0.71802786]\n",
      "Predictions:  [-1  1  1  1  1  1 -1  1 -1 -1]\n",
      "\n",
      "### Starting training  3\n",
      "Epoch 1 >> W = [-1.          0.2570316   0.96509117  0.18488782]\n",
      "Epoch 416 >> W = [ 3.          1.5364316   2.44869117 -0.70071218]\n",
      "Predictions:  [-1  1  1  1  1  1 -1  1 -1 -1]\n",
      "\n",
      "### Starting training  4\n",
      "Epoch 1 >> W = [-1.          0.34629129  0.44733042  0.45380844]\n",
      "Epoch 409 >> W = [ 2.98        1.53369129  2.43533042 -0.69659156]\n",
      "Predictions:  [-1  1  1  1  1  1 -1  1 -1 -1]\n",
      "\n",
      "### Starting training  5\n",
      "Epoch 1 >> W = [-1.          0.63720448  0.07353498  0.46367355]\n",
      "Epoch 461 >> W = [ 3.12        1.61240448  2.54213498 -0.73152645]\n",
      "Predictions:  [-1  1  1  1  1  1 -1  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "for training_index in np.arange(1, 6, 1):\n",
    "    print (\"\\n### Starting training \", training_index)\n",
    "    p = Perceptron(activation='bipolar_step', learning_rate=0.01)        \n",
    "    p.train(features=x_train, target=y_train, max_epochs=2000)      \n",
    "    print (\"Predictions: \", p.predict(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training and prediction 5 times the algorithms on the given train and test set, we can see that, for each round, the output can be different. This is due to the fact that the weights of the perceptron are initialized randomly, which also yields to a different number of training epochs as shown in the log above. \n",
    "\n",
    "Given that the number of epochs did not reach the maximum number of epochs stablished in the model initialization (2000), we can conclude that the perceptron did manage to separate all classes from the train set. As a result, it is possible to affirm that such classes are linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
