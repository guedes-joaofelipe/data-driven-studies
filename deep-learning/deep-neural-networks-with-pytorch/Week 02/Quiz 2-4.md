### Gradient Descent

1. In gradient descent, what happens when you select a learning rate that is too large? 

- >You may miss the minimum and your loss will start increasing 
- It may take to long to converge to a minimum 
- The loss function will become concave 

2. How do you select an initial parameter value for the first iteration of gradient descent?

- Set it to 100
- It should always be 0
- >Randomly